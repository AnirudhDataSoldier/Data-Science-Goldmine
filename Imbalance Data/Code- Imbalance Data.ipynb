{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Imbalance Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "1. <a href = \"#1.-Imbalance-Data\"> Imbalance Data</a>\n",
    "2. <a href = \"#2.-Techniques-used\">Techniques Used</a>\n",
    "3. <a href = \"#3.-Random-under-sampling\">Random Under Sampling</a>\n",
    "            - Random Under Sampling \n",
    "            - Tomek Lines\n",
    "            - Cluster Centroid under-sampling\n",
    "            - Near Miss under-sampling\n",
    "4. <a href = \"#4.-Random-over-sampling\">Random Over Sampling</a>\n",
    "            - Random Over Sampling\n",
    "5. <a href = \"#5.-Synthetic-data-generation\">Synthetic data generation</a>\n",
    "            - Synthetic Minority Oversampling Technique (SMOTE)\n",
    "            - Adaptive Synthetic Technique (ADASYN)\n",
    "6. <a href = \"#6.-Ensemble-methods\">Ensemble Methods</a>\n",
    "            - Balanced Bagging Classifier\n",
    "            - RUS Boosting Classifier\n",
    "            - Easy Ensemble Classifier\n",
    "7. <a href = \"#7.-Conclusion\">Conclusion</a>\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imbalance Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Machine Learning we often come across a term called Imbalanced Data Distribution, where the number of observations belonging to one class is significantly lower than those belonging to the other classes. ML algorithms tend to produce unsatisfactory classifiers when faced with imbalanced datasets. For any imbalanced data set, if the event to be predicted belongs to the minority class and the event rate is less than 5%, it is usually referred to as a rare event.\n",
    "\n",
    "**Examples of Imbalance Data**\n",
    "- Finance: Fraud detection datasets commonly have a fraud rate of ~1–2%\n",
    "- Telecom: Classifying probable churning customers to reduce the churn rate\n",
    "- Transportation/Airline: Will Airplane failure occur\n",
    "- Medical: Does a patient has cancer\n",
    "\n",
    "**Why traditional ML models does not work well with Imbalance Datasets** \n",
    "\n",
    "Machine Learning Algorithms are usually designed to improve accuracy by reducing the error. Thus, they do not take into account the class distribution / proportion or balance of classes.\n",
    "Standard ML techniques such as Decision Tree and Logistic Regression have a bias towards the majority class, and they tend to ignore the minority class. They tend only to predict the majority class, hence, having major misclassification of the minority class in comparison with the majority class. In more technical words, if we have imbalanced data distribution in our dataset then our model becomes more prone to the case when minority class has negligible or very lesser recall.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Techniques used\n",
    "\n",
    "\n",
    "A widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and / or adding more examples from the minority class (over-sampling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random under-sampling\n",
    "\n",
    "Undersampling can be defined as removing some observations of the majority class. This is done until the majority and minority class is balanced out.\n",
    "\n",
    "Undersampling can be a good choice when you have a ton of data -think millions of rows. But a drawback to undersampling is that we are removing information that may be valuable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![undersampling.png](undersampling.png)\n",
    "\n",
    "In under-sampling, the simplest technique involves removing random records from the majority class, which can cause loss of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages:\n",
    "- If the dataset is huge, we might face run time and storage problems. Undersampling can help to handle these problems successfully by improving run time and storage problems by reducing the number of training data samples.\n",
    "\n",
    "#### Disadvantages:\n",
    "- This method can discard potentially useful information which could be important for building the classifiers.\n",
    "- The sample chosen by random under sampling may be a biased one. It may not be an accurate representation of the population. So, it results in inaccurate results with the actual dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Implementation of Random Under Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import Python libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "%matplotlib inline\n",
    "\n",
    "#Import dataset\n",
    "#Dataset Link: https://www.kaggle.com/mlg-ulb/creditcardfraud\n",
    "\n",
    "basepath = '../../Data\\\\creditcard.csv'\n",
    "df = pd.read_csv(basepath)\n",
    "# check dataset\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the distribution of Class column\n",
    "\n",
    "df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Total Transaction    = 284,807 \n",
    "    Fraudulent trans     = 492\n",
    "    Non-Fraudulent trans = 284315 \n",
    "\n",
    "\n",
    "Now, Calculating the percentage of labels 0 and 1 within the `Class` column.\n",
    "\n",
    "    0    99.8273%\n",
    "    1    0.1727%\n",
    "\n",
    "We can see that the `Class` column is highly imbalanced. It contains 99.82% labels as `0` and 0.17% labels as `1`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KVSH2\\AppData\\Local\\Temp\\1\\ipykernel_25508\\3478965371.py:4: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  (df['Class'].value_counts()/np.float(len(df))).plot.bar()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# view the distribution of percentages within the Class column\n",
    "\n",
    "\n",
    "(df['Class'].value_counts()/np.float(len(df))).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the minority and majority classes\n",
    "df_majority = df[df['Class']==0]\n",
    "df_minority = df[df['Class']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying UnderSampling\n",
    "# undersample majority class\n",
    "\n",
    "df_majority_undersampled = resample(df_majority, replace=True, n_samples=492, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine majority class with oversampled minority class\n",
    "\n",
    "df_undersampled = pd.concat([df_minority, df_majority_undersampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    492\n",
       "0    492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display new class value counts\n",
    "\n",
    "df_undersampled['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see that the new dataframe `df_undersampled` has fewer observations than the original one `df` and the ratio of the two classes is now 1:1.\n",
    "\n",
    "Again, I will train a model using Logistic Regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 93.19%\n"
     ]
    }
   ],
   "source": [
    "# Model Building\n",
    "\n",
    "# declare feature vector and target variable\n",
    "X2 = df_undersampled.drop(['Class'], axis=1)\n",
    "y2 = df_undersampled['Class']\n",
    "\n",
    "# import Logistic Regression classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# instantiate the Logistic Regression classifier\n",
    "logreg2 = LogisticRegression()\n",
    "\n",
    "\n",
    "# fit the classifier to the imbalanced data\n",
    "clf2 = logreg2.fit(X2, y2)\n",
    "\n",
    "\n",
    "# predict on the training data\n",
    "y2_pred = clf2.predict(X2)\n",
    "\n",
    "\n",
    "# print the accuracy\n",
    "accuracy2 = accuracy_score(y2_pred, y2)\n",
    "\n",
    "print(\"Accuracy : %.2f%%\" % (accuracy2 * 100.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random under sampling using Algorithm\n",
    "\n",
    "There is a Python library which enable us to handle the imbalanced datasets. It is called **Imbalanced-Learn**.      \n",
    "    **For Installation:  pip install imbalanced-learn**\n",
    "\n",
    "Then, I will import the `RandomUnderSampler` class. It is a quick and easy way to balance the data by randomly selecting a subset of data for the targeted classes. \n",
    "\n",
    "[PyPi Link](https://pypi.org/project/imbalanced-learn/#id26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.9.1-py3-none-any.whl (199 kB)\n",
      "     -------------------------------------- 199.3/199.3 kB 1.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\program files\\python39\\lib\\site-packages (from imbalanced-learn->imblearn) (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\program files\\python39\\lib\\site-packages (from imbalanced-learn->imblearn) (1.23.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\program files\\python39\\lib\\site-packages (from imbalanced-learn->imblearn) (1.9.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\program files\\python39\\lib\\site-packages (from imbalanced-learn->imblearn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\program files\\python39\\lib\\site-packages (from imbalanced-learn->imblearn) (1.2.0)\n",
      "Installing collected packages: imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.9.1 imblearn-0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import imbalanced learn library\n",
    "# pip install imbalanced-learn\n",
    "\n",
    "import imblearn\n",
    "\n",
    "# import RandomUnderSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# instantiate the RandomUnderSampler\n",
    "rus = RandomUnderSampler()\n",
    "\n",
    "# declare feature vector and target variable\n",
    "\n",
    "X = df.drop(['Class'], axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# fit the RandomUnderSampler to the dataset\n",
    "X_rus, y_rus = rus.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tomek Links\n",
    "\n",
    "Tomek links are defined as the two observations of different classes which are nearest neighbours of each other.\n",
    "\n",
    "The figure below illustrate the concept of Tomek links-\n",
    "\n",
    "\n",
    "![tomek.png](attachment:tomek.png)\n",
    "\n",
    "\n",
    "\n",
    "We can see in the above image that the Tomek links (circled in green) are given by the pairs of red and blue data points that are nearest neighbors. Most of the classification algorithms face difficulty due to these points. So, I will remove these \n",
    "points and increase the separation gap between two classes.  Now, the algorithms produce more reliable output.\n",
    "\n",
    "This technique will not produce a balanced dataset. It will simply clean the dataset by removing the Tomek links. It may result in an easier classification problem. Thus, by removing the Tomek links, we can improve the performance of the classifier even if we don’t have a balanced dataset.\n",
    "\n",
    "\n",
    "So, removing the Tomek links increases the gap between the two classes and thus facilitate the classification process.\n",
    "\n",
    "**Advantages**\n",
    "- It is quite conservative; that is, it is good at finding and removing points we are quite certain are outliers (though it pays to inspect the result yourself afterwards, just to be sure the right things were removed).\n",
    "\n",
    "**Disadvantages**\n",
    "- Not easily tunable\n",
    "- Sometimes it retains outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "tl = TomekLinks()\n",
    "X_tl, y_tl = tl.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284736, 30) (284736,)\n"
     ]
    }
   ],
   "source": [
    "print(X_tl.shape, y_tl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Centroids\n",
    "\n",
    "\n",
    "In this technique, we perform under-sampling by generating centroids based on clustering methods. The dataset will be grouped\n",
    "by similarity, in order to preserve information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283823 New points undersampled under Cluster Centroids\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import ClusterCentroids\n",
    "cc = ClusterCentroids()\n",
    "X_cc, y_cc = cc.fit_resample(X, y)\n",
    "\n",
    "print(X.shape[0] - X_cc.shape[0], 'New points undersampled under Cluster Centroids')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Near miss undersampling\n",
    "[Reference Link](https://imbalanced-learn.org/dev/auto_examples/under-sampling/plot_illustration_nearmiss.html#sphx-glr-auto-examples-under-sampling-plot-illustration-nearmiss-py)\n",
    "\n",
    "In order to tackle the issue of potential information loss, near-neighbor method and its variations have been proposed and widely used. When instances of two different classes are very close to each other, we remove the instances of the majority class to increase the spaces between the two classes. This helps in the classification process.\n",
    "\n",
    "In near miss undersampling, we only sample the data points from the majority class which are necessary to distinguish the majority class from other classes.\n",
    "\n",
    "The basic intuition about the working of near-neighbor methods is as follows:\n",
    "\n",
    "* Step 1: The method first finds the distances between all instances of the majority class and the instances of the minority class. Here, majority class is to be under-sampled.\n",
    "* Step 2: Then, n instances of the majority class that have the smallest distances to those in the minority class are selected.\n",
    "* Step 3: If there are k instances in the minority class, the nearest method will result in k*n instances of the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NearMiss-1\n",
    "\n",
    "NearMiss-1 selects samples from the majority class for which the average distance to some nearest neighbours is the smallest. In the following example, we use a 3-NN to compute the average distance on 2 specific samples of the majority class. Therefore, in this case the point linked by the green-dashed line will be selected since the average distance is smaller.\n",
    "\n",
    "![nearmiss1.png](attachment:nearmiss1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NearMiss-2\n",
    "\n",
    "NearMiss-2 selects samples from the majority class for which the average distance to the farthest neighbors is the smallest. With the same configuration mentioned above that the sample linked to the green-dashed line will be selected since its distance the 3 farthest neighbors is the smallest.\n",
    "\n",
    "![nearmiss2.png](attachment:nearmiss2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous size (284807, 30) NearMiss Sample (984, 30)\n",
      "\n",
      "Here, the majority class has been reduced to the total number of minority class, so that both classes will have equal number of records.\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "nr = NearMiss()\n",
    "X_nm, y_nm = nr.fit_resample(X, y)\n",
    "print(\"Previous size\",X.shape, \"NearMiss Sample\",X_nm.shape )\n",
    "\n",
    "print(\"\\nHere, the majority class has been reduced to the total number of minority class, so that both classes will have equal number of records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[492 492]\n",
      "\n",
      "NearMiss has created a sample with target variable in the ratio of 1:1\n"
     ]
    }
   ],
   "source": [
    "print(np.bincount(y_nm))\n",
    "print(\"\\nNearMiss has created a sample with target variable in the ratio of 1:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random over-sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![oversampling.png](oversampling.png)\n",
    "\n",
    "The Oversampling methods work with the minority class. In these methods, we duplicate random instances of the minority class. So, it replicates the observations from minority class to balance the data. It is also known as upsampling. It may result in overfitting due to duplication of data points.\n",
    "The simplest implementation of over-sampling is to duplicate random records from the minority class, which can cause overfitting.\n",
    "\n",
    "**Random oversampling**\n",
    "\n",
    "In random oversampling, we balance the data by randomly oversampling the minority class.\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "* An advantage of this method is that it leads to no information loss.\n",
    "\n",
    "* This method outperform under sampling.\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "* This method increases the likelihood of overfitting as it replicates the minority class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 30) (568630, 30) (568630,)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler()\n",
    "\n",
    "X_ros, y_ros = ros.fit_resample(X, y)\n",
    "\n",
    "print(X.shape, X_ros.shape, y_ros.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283823 new random points generated\n"
     ]
    }
   ],
   "source": [
    "print(X_ros.shape[0] - X.shape[0], 'new random points generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Synthetic data generation\n",
    "\n",
    "### (SMOTE - Synthetic Minority Oversampling Technique)\n",
    "\n",
    "SMOTE is an over-sampling method. What it does is, it creates synthetic (not duplicate) samples of the minority class. Hence making the minority class equal to the majority class. SMOTE does this by selecting similar records and altering that record one column at a time by a random amount within the difference to the neighbouring records.\n",
    "\n",
    "In the context of **synthetic data generation** , there is a powerful and widely used method known as synthetic minority oversampling technique or SMOTE. Under this technique, artificial data is created based on feature space. Artificial data is generated with bootstrapping and k-nearest neighbours algorithm. It works as follows:-\n",
    "\n",
    "1. First of all, we take the difference between the feature vector (sample) under consideration and its nearest neighbour.\n",
    "\n",
    "2. Then we multiply this difference by a random number between 0 and 1.\n",
    "\n",
    "3. Then we add this number to the feature vector under consideration.\n",
    "\n",
    "4. Thus we select a random point along the line segment between two specific features.\n",
    "\n",
    "So, SMOTE generates new observations by interpolation between existing observations in the dataset.\n",
    "\n",
    "![smote.png](smote.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([284315, 284315], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smt = SMOTE()\n",
    "X_smote, y_smote = smt.fit_resample(X, y)\n",
    "np.bincount(y_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283823 new random points generated\n"
     ]
    }
   ],
   "source": [
    "print(X_smote.shape[0] - X.shape[0], 'new random points generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADASYN vs SMOTE\n",
    "\n",
    "* ADASYN generates more synthetic data from samples in the minority class, that are harder to classify and less synthetic data where it's less harder to classify, whereas in SMOTE, there is a uniform weight for all minority points.\n",
    "\n",
    "* SMOTE uses only the minority class to train the KNN, on the contrary ADASYN uses all the samples to train the KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Synthetic Technique or ADASYN\n",
    "\n",
    "This technique works in a similar way as SMOTE. But the number of samples generated is proportional to the number of nearby samples which do not belong to the same class. Thus it focusses on outliers when generating the new training samples.\n",
    "\n",
    "**Advantages**\n",
    "* Its adaptive nature of creating more data for “harder-to-learn” examples and allowing you to sample more negative data for the model.\n",
    "* ADASYN can ultimately synthetically balance your data set.\n",
    "\n",
    "**Disadvantages**\n",
    "* For minority examples that are sparsely distributed, each neighbourhood may only contain 1 minority example.\n",
    "* Precision of ADASYN may suffer due to adaptability nature.\n",
    "\n",
    "[Reference Link](https://medium.com/@ruinian/an-introduction-to-adasyn-with-code-1383a5ece7aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([284315, 284298], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "ada = ADASYN(random_state=42)\n",
    "X_ada, y_ada = ada.fit_resample(X, y)\n",
    "np.bincount(y_ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283806 new random points generated\n"
     ]
    }
   ],
   "source": [
    "print(X_ada.shape[0] - X.shape[0], 'new random points generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ensemble methods\n",
    "\n",
    "In this section, we will take a look at an alternative approach to deal with imbalanced datasets. In this approach, we modify the existing classification algorithms to make them appropriate for imbalanced datasets.\n",
    "\n",
    "In this approach, we construct several two stage classifiers from the original data and then we aggregate their predictions. The main aim of this ensemble technique is to improve the performance of single classifiers.\n",
    "\n",
    "The ensemble technique are of two types - bagging and boosting. These techniques are discussed below:-\n",
    "\n",
    "#### Bagging\n",
    "\n",
    "Bagging is an abbreviation of Bootstrap Aggregating. In the conventional bagging algorithm, we generate n different bootstrap training samples with replacement. Then we train the algorithm on each bootstrap training samples separately and then aggregate the predictions at the end. Bagging is used to reduce overfitting in order to create strong learners so that we can generate strong predictions. Bagging allows replacement in the bootstrapped training sample.\n",
    "\n",
    "The machine learning algorithms like logistic regression, decision tree and neural networks are fitted to each bootstrapped training sample. These classifiers are then aggregated to produce a compound classifier. This ensemble technique produces a strong compound classifier since it combines individual classifiers to come up with a strong classifier.\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "* This technique improves stability and accuracy of machine learning algorithms.\n",
    "\n",
    "* It reduces variance and overcomes overfitting.\n",
    "\n",
    "* It improves misclassification rate of the bagged classifier.\n",
    "\n",
    "* In noisy data situations bagging outperforms boosting.\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "* Bagging works only if the base classifiers are not bad to begin with. Bagging with bad classifiers can further degrade the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[69699  1383]\n",
      " [   14   106]]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Splitting data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=0)\n",
    "\n",
    "\n",
    "bbc = BalancedBaggingClassifier(random_state=42)\n",
    "\n",
    "bbc.fit(X_train, y_train)\n",
    "y_pred = bbc.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "#### RUSBoostClassifier\n",
    "\n",
    "Boosting is an ensemble technique to combine weak learners to create a strong learner so that we can make accurate predictions. In boosting, we start with a base or weak classifier that is prepared on the training data.\n",
    "\n",
    "The base learners are weak learners. So, the prediction accuracy is only slightly better than average. A classifier learning algorithm is said to be weak when small changes in data results in big changes in the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "clf = RUSBoostClassifier(random_state=0)\n",
    "clf.fit(X, y) \n",
    "clf.predict(X) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EasyEnsembleClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[68756  2326]\n",
      " [   11   109]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.ensemble import EasyEnsembleClassifier \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=0)\n",
    "\n",
    "eec = EasyEnsembleClassifier(random_state=42)\n",
    "eec.fit(X_train, y_train) \n",
    "\n",
    "y_pred = eec.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "\n",
    "We have discussed various approaches to deal with the problem of imbalanced classes. These are random oversampling, random undersampling, tree-based algorithms, resampling with imbalanced learn library, under-sampling : Tomek links, under-sampling : Cluster Centroids and over-sampling : SMOTE.\n",
    "\n",
    "Some combination of these approaches will help us to create a better classifier. Simple sampling techniques may handle slight imbalance whereas more advanced methods like ensemble methods are required for extreme imbalances. The most effective technique will vary according to the dataset.\n",
    "\n",
    "So, based on the above discussion, we can conclude that there is no one solution to deal with the imbalanced classes problem. We should try out multiple methods to select the best-suited sampling techniques for the dataset in hand. The most effective technique will vary according to the characteristics of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reference Link](https://www.analyticsvidhya.com/blog/2021/05/feature-engineering-how-to-detect-and-remove-outliers-with-python-code/)\n",
    "* https://imbalanced-learn.org/en/stable/index.html\n",
    "* https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/\n",
    "* https://www.analyticsvidhya.com/blog/2017/03/imbalanced-data-classification/\n",
    "* https://elitedatascience.com/imbalanced-classes\n",
    "* https://www.geeksforgeeks.org/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python/\n",
    "* https://www.kaggle.com/souravsaha1605/comprehensive-guide-on-imbalanced-data-handling\n",
    "* https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tested:no errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
