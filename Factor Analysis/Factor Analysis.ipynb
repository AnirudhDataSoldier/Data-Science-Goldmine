{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/10/dimensionality-reduction-using-factor-analysis-in-python/\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/introduction-factor-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Factor Analysis?\n",
    "\n",
    ">Factor analysis is one of the unsupervised machine learning algorithms which is used for dimensionality reduction. Factor analysis is a linear statistical model. It is used to explain the variance among the observed variable and condense a set of the observed variable into the unobserved variable called factors. Observed variables are modeled as a linear combination of factors and error terms. Factor or latent variable is associated with multiple observed variables, who have common patterns of responses. Each factor explains a particular amount of variance in the observed variables. It helps in data interpretations by reducing the number of variables.\n",
    "\n",
    ">Factor Analysis (FA) is an exploratory data analysis method used to search influential underlying factors or latent variables from a set of observed variables. It helps in data interpretations by reducing the number of variables. It extracts maximum common variance from all variables and puts them into a common score.\n",
    "\n",
    "What is Factor?\n",
    "\n",
    ">A factor is a latent variable which describes the association among the number of observed variables. The maximum number of factors are equal to a number of observed variables. Every factor explains a certain variance in observed variables. The factors with the lowest amount of variance were dropped. Factors are also known as latent variables or hidden variables or unobserved variables or Hypothetical variables.\n",
    "\n",
    "Uses of Factor Analysis:\n",
    "\n",
    ">Factor analysis is widely utilized in market research, advertising, psychology, finance, and operation research. Market researchers use factor analysis to identify price-sensitive customers, identify brand features that influence consumer choice, and helps in understanding channel selection criteria for the distribution channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the types of Factor Analysis?\n",
    "\n",
    ">There are 2 types of Factor Analysis:\n",
    "- Exploratory Factor Analysis: It is the most popular factor analysis approach among social and management researchers. Its basic assumption is that any observed variable is directly associated with any factor.\n",
    "- Confirmatory Factor Analysis (CFA): Its basic assumption is that each factor is associated with a particular set of observed variables. CFA confirms what is expected on the basic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does Factor Analysis work?\n",
    "\n",
    ">The primary objective of factor analysis is to reduce the number of observed variables and find unobservable variables. These unobserved variables help the market researcher to conclude the survey. This conversion of the observed variables to unobserved variables can be achieved in the following steps:\n",
    "\n",
    "- Factor Extraction: In this step, the number of factors and approach for extraction selected using variance partitioning methods such as principal components analysis and common factor analysis.\n",
    "- Factor Rotation: Rotation is a tool for better interpretation of factor analysis. Rotation can be orthogonal or oblique. It re-distributed the commonalities with a clear pattern of loadings. In this step, rotation tries to convert factors into uncorrelated factors — the main goal of this step to improve the overall interpretability. There are lots of rotation methods that are available such as: Varimax rotation method, Quartimax rotation method, and Promax rotation method.\n",
    "\n",
    "Steps of Factor Analysis:\n",
    "\n",
    "- Bartlett’s Test of Sphericity and KMO Test\n",
    "- Determining the number of factors\n",
    "- Interpreting the factors\n",
    "\n",
    "Before proceeding with Factor Analysis, we need to take care of following steps with the data:\n",
    "\n",
    "- There are no outliers in data.\n",
    "- Sample size should be greater than the factor.\n",
    "- There should not be perfect multicollinearity.\n",
    "- There should not be homoscedasticity between the variables.\n",
    "- The data should be Standard scaled. \n",
    "- The features have to be numeric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Statement:\n",
    "\n",
    ">The inner beauty is always cherished rather than the outer appearance. These lines show that the character of a person is more important rather than his appearance. Business firms these days take these into considerations and aim at selecting a person rather than a talent employee.Thus, deciding the personality of a person becomes necessary for firms to increase their productivity. Here, we are given the scores for various personalities of a person we try to reduce them and bring the unobserved feature or behavior into consideration. This can be done with the help of dimension reduction techniques such as the Factor Analysis. The dataset had scores for various personalities for a person ranging from 1 to 10. The various personalities given are \"distant\", \"talkatv\", \"carelss\", \"hardwrk\", \"anxious\",\"agreebl\", \"tense\", \"kind\", \"opposng\", \"relaxed”,\"disorgn\", \"outgoin\", \"approvn\", \"shy\", \"discipl\",\"harsh\", \"persevr\", \"friendl\", \"worryin\", \"respnsi\",\"contrar\", \"sociabl\", \"lazy\", \"coopera\", \"quiet\",\"organiz\", \"criticl\", \"lax\", \"laidbck\", \"withdrw\",\"givinup\", \"easygon”.The dataset had about 292 instances.\n",
    "\n",
    "Approach:\n",
    "\n",
    ">Initially, the data is checked for any null values. Later, the data are scaled using the standard scaling technique. Then, the scaled data are passed through various tests such as the Bartlett’s test of sphericity and the KMO test to determine whether the dimensionality reduction techniques such as the Factor Analysis can be applied on this dataset. With the help of Scree plot,\n",
    "the optimal number of factors are determined. Then the Factor Analysis is implemented using the Factor Analysis Module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <a href = \"#1.-Loading-the-Dataset\">Loading the Dataset</a>\n",
    "2. <a href = \"#2.-Data-Preprocessing\">Data Preprocessing</a>\n",
    "3. <a href = \"#3.-Adequacy-Test\">Adequacy Test</a>\n",
    "    - Bartlett's Test of Sphericity\n",
    "    - Kaiser-Meyer-Olkin (KMO) Test\n",
    "4. <a href = \"#4.-Determining-the-Number-of-Factors\">Determining the Number of Factors</a>\n",
    "5. <a href = \"#5.-Interpreting-the-Factors\">Interpreting the Factors</a>\n",
    "    - Loadings\n",
    "    - Variance\n",
    "    - Communalities\n",
    "6. <a href = \"#6.-Inference\">Inference</a>\n",
    "7. <a href = \"#7.-Pros-and-Cons-of-Factor-Analysis\">Pros and Cons of Factor Analysis</a>\n",
    "8. <a href = \"#8.-Factor-Analysis-Vs.-Principle-Component-Analysis\">Factor Analysis Vs. Principle Component Analysis</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity,calculate_kmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: factor_analyzer in c:\\program files\\python39\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: pandas in c:\\program files\\python39\\lib\\site-packages (from factor_analyzer) (1.5.0)\n",
      "Requirement already satisfied: pre-commit in c:\\program files\\python39\\lib\\site-packages (from factor_analyzer) (2.20.0)\n",
      "Requirement already satisfied: numpy in c:\\program files\\python39\\lib\\site-packages (from factor_analyzer) (1.23.3)\n",
      "Requirement already satisfied: scipy in c:\\program files\\python39\\lib\\site-packages (from factor_analyzer) (1.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\program files\\python39\\lib\\site-packages (from factor_analyzer) (1.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\program files\\python39\\lib\\site-packages (from pandas->factor_analyzer) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\program files\\python39\\lib\\site-packages (from pandas->factor_analyzer) (2022.2.1)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in c:\\program files\\python39\\lib\\site-packages (from pre-commit->factor_analyzer) (3.3.1)\n",
      "Requirement already satisfied: identify>=1.0.0 in c:\\program files\\python39\\lib\\site-packages (from pre-commit->factor_analyzer) (2.5.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\program files\\python39\\lib\\site-packages (from pre-commit->factor_analyzer) (6.0)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in c:\\program files\\python39\\lib\\site-packages (from pre-commit->factor_analyzer) (1.7.0)\n",
      "Requirement already satisfied: virtualenv>=20.0.8 in c:\\program files\\python39\\lib\\site-packages (from pre-commit->factor_analyzer) (20.16.5)\n",
      "Requirement already satisfied: toml in c:\\program files\\python39\\lib\\site-packages (from pre-commit->factor_analyzer) (0.10.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\program files\\python39\\lib\\site-packages (from scikit-learn->factor_analyzer) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\program files\\python39\\lib\\site-packages (from scikit-learn->factor_analyzer) (1.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python39\\lib\\site-packages (from nodeenv>=0.11.1->pre-commit->factor_analyzer) (56.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\program files\\python39\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->factor_analyzer) (1.16.0)\n",
      "Requirement already satisfied: distlib<1,>=0.3.5 in c:\\program files\\python39\\lib\\site-packages (from virtualenv>=20.0.8->pre-commit->factor_analyzer) (0.3.6)\n",
      "Requirement already satisfied: filelock<4,>=3.4.1 in c:\\program files\\python39\\lib\\site-packages (from virtualenv>=20.0.8->pre-commit->factor_analyzer) (3.8.0)\n",
      "Requirement already satisfied: platformdirs<3,>=2.4 in c:\\program files\\python39\\lib\\site-packages (from virtualenv>=20.0.8->pre-commit->factor_analyzer) (2.5.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install factor_analyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\KVSH2\\\\Novartis Pharma AG\\\\Data Insights and Analytics (DiA) Team - DS Code Repository\\\\Code\\\\Factor AnalysisFactor Analysis\\\\Standford.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFactor Analysis\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mStandford.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd()\u001b[38;5;241m+\u001b[39mfile\n\u001b[1;32m----> 7\u001b[0m fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m);\n\u001b[0;32m      8\u001b[0m header \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mreadline()\n\u001b[0;32m      9\u001b[0m data\u001b[38;5;241m=\u001b[39m[]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\KVSH2\\\\Novartis Pharma AG\\\\Data Insights and Analytics (DiA) Team - DS Code Repository\\\\Code\\\\Factor AnalysisFactor Analysis\\\\Standford.txt'"
     ]
    }
   ],
   "source": [
    "#Reading the Dataset\n",
    "#basepath='../../Data\\\\'\n",
    "#with open('C:\\Somashree\\Project\\DS Code Repository\\Factor Analysis\\Standford.txt','r') as file:\n",
    "#with open(basepath+'Standford.txt','r') as file:\n",
    "file = \"Factor Analysis\\Standford.txt\"\n",
    "path = os.getcwd()+file\n",
    "fp = open(path, 'r+');\n",
    "header = file.readline()\n",
    "data=[]\n",
    "for row in file.readlines()[1:]:\n",
    "    row = row.split()[1:]\n",
    "    data.append(row)\n",
    "data = np.array(data,dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(data,columns=np.array(header.split(),dtype=object))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(dataset.columns),dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<a href=\"#Content\">Back to Content</a>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler =  StandardScaler()\n",
    "dataframe = scaler.fit_transform(dataset)\n",
    "dataframe = pd.DataFrame(data=dataframe,columns=dataset.columns)\n",
    "dataframe.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<a href=\"#Content\">Back to Content</a>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adequacy Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaled data are passed through various tests to determine whether the dimensionality reduction techniques such as the Factor Analysis can be applied on this dataset. There are two methods to check the factorability or sampling adequacy:\n",
    "- Bartlett's Test of Sphericity\n",
    "- Kaiser-Meyer-Olkin Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bartlett's Test of Sphericity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bartlett’s test checks whether the correlation is present in the given data. It tests the null hypothesis (H0) that the correlation matrix is an Identical matrix. The identical matrix consists of all the diagonal elements as 1. So, the null hypothesis assumes that no correlation is present among the variables.\n",
    "\n",
    "We want to reject this null hypothesis because factor analysis aims at explaining the common variance i.e. the variation due to correlation among the variables. If the p test statistic value is less than 0.05, we can decide that the correlation is not an Identical matrix i.e. correlation is present among the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2,p = calculate_bartlett_sphericity(dataframe)\n",
    "print(\"Bartlett Sphericity Test\")\n",
    "print(\"Chi squared value : \",chi2)\n",
    "print(\"p value : \",p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Since the p test statistic is less than 0.05, we can conclude that correlation is present among the variables which is a green signal to apply factor analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaiser-Meyer-Olkin (KMO) Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMO Test measures the proportion of variance that might be a common variance among the variables. Larger proportions are expected as it represents more correlation is present among the variables thereby giving way for the application of dimensionality reduction techniques such as Factor Analysis. KMO score is always between 0 to 1 and values more than 0.6 are much appreciated. We can also say it as a measure of how suited our data is for factor analysis. \n",
    "Just pass the dataframe which contains information about the dataset to the calculate_kmo function. The function will return the proportion of variance for each variable which is stored in the variable ‘kmo_vars’ and the proportion of variance for the whole of our data is stored in ‘kmo_model’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmo_all,kmo_model = calculate_kmo(dataset)\n",
    "print(\"KMO Test Statisitc\",kmo_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">We can see that our data has an overall proportion of variance of 0.84. It shows that our data has more correlation and dimensionality reduction techniques such as the factor analysis can be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<a href=\"#Content\">Back to Content</a>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Determining the Number of Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of factors in our dataset is equal to the number of variables in our dataset. All the factors are not gonna provide a significant amount of useful information about the common variance among the variables. So we have to decide the number of factors. The number of factors can be decided on the basis of the amount of common variance the factors explain. In general, we will plot the factors and their eigenvalues.\n",
    "\n",
    "**Eigenvalues** are nothing but the amount of variance the factor explains. It represent variance explained each factor from the total variance. We will select the number of factors whose eigenvalues are greater than 1. It is also known as characteristic roots.\n",
    "\n",
    "But why should we choose the factors whose eigenvalues are greater than 1? \n",
    "\n",
    ">In a standard normal distribution with mean 0 and Standard deviation 1, the variance will be 1. Since we have standard scaled the data the variance of a feature is 1. This is the reason for selecting factors whose eigenvalues(variance) are greater than 1 i.e. the factors which explain more variance than a single observed variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = FactorAnalyzer(rotation = None,impute = \"drop\",n_factors=dataframe.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa.fit(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev,_ = fa.get_eigenvalues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(1,dataframe.shape[1]+1),ev)\n",
    "plt.plot(range(1,dataframe.shape[1]+1),ev)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Factors')\n",
    "plt.ylabel('Eigen Value')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The eigenvalues function will return the original eigenvalues and the common factor eigenvalues. Now, we are going to consider only the original eigenvalues. From the graph, we can see that the eigenvalues drop below 1 from the 7th factor. So, the optimal number of factors is 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<a href=\"#Content\">Back to Content</a>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interpreting the Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an optimal number of factors i.e. 6 in our case. Then, we have to interpret the factors by making use of:\n",
    "- Loadings\n",
    "- Variance\n",
    "- Communalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The factor loading is a matrix which shows the relationship of each variable to the underlying factor. It shows the correlation coefficient for observed variable and factor. It shows the variance explained by the observed variables. Loadings indicate how much a factor explains a variable. The loading score will range from -1 to 1.Values close to -1 or 1 indicate that the factor has an influence on these variables. Values close to 0 indicates that the factor has a lower influencer on the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = FactorAnalyzer(n_factors=6,rotation='varimax')\n",
    "fa.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(fa.loadings_,index=dataframe.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In Factor 0, we can see that the features ‘distant’ and ‘shy’ talkative have high loadings than other variables. From this, we can see that Factor 0, explains the common variance in people who are reserved i.e. the variance among the people who are distant and shy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount of variance explained by each factor can be found out using the ‘get_factor_variance’ function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(fa.get_factor_variance(),index=['Variance','Proportional Var','Cumulative Var']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The first row represents the variance explained by each factor. Proportional variance is the variance explained by a factor out of the total variance. Cumulative variance is nothing but the cumulative sum of proportional variances of each factor. In our case, the 6 factors together are able to explain 55.3% of the total variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Communalities are the sum of the squared loadings for each variable. It represents the common variance. It ranges from 0-1 and value close to 1 represents more variance.Communality is the proportion of each variable’s variance that can be explained by the factors. Rotations don’t have any influence over the communality of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(fa.get_communalities(),index=dataframe.columns,columns=['Communalities']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The proportion of each variable’s variance that is explained by the factors can be inferred from the above. For example, we could consider the variable ‘talkatv’ about 62.9% of its variance is explained by all the factors together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<a href=\"#Content\">Back to Content</a>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bartlett’s test of sphericity had a p test statistic of 0.0 at 95% confidence which states\n",
    "that the correlation matrix is not an identity matrix i.e. correlation is present among the variables.\n",
    "The overall KMO statistic value is 0.88 which states the sampling is adequate and thus providing\n",
    "the way for applying factor analysis. The optimal number of factors is 6 as their eigen values are\n",
    "above 1 which can also be inferred from the scree plot. Factor explains creates factor which can\n",
    "explain the amount of variance due to correlation among the variables. The 6 factors\n",
    "cumulatively explain about 55% of the common variance where the factor 1 leads with explaining\n",
    "about 14% of the common variance. Thus, factor analysis has helped us reducing the dimensions\n",
    "by introducing factors where each factor has helped in explaining the variance due to the\n",
    "correlation among the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<a href=\"#Content\">Back to Content</a>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pros and Cons of Factor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factor analysis explores large dataset and finds interlinked associations. It reduces the observed variables into a few unobserved variables or identifies the groups of inter-related variables, which help the market researchers to compress the market situations and find the hidden relationship among consumer taste, preference, and cultural influence. Also, it helps in improve questionnaire in for future surveys. Factors make for more natural data interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<a href=\"#Content\">Back to Content</a>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Factor Analysis Vs. Principle Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCA components explain the maximum amount of variance while factor analysis explains the covariance in data.\n",
    "- PCA components are fully orthogonal to each other whereas factor analysis does not require factors to be orthogonal.\n",
    "- PCA component is a linear combination of the observed variable while in FA, the observed variables are linear combinations of the unobserved variable or factor.\n",
    "- PCA components are uninterpretable. In FA, underlying factors are labelable and interpretable.\n",
    "- PCA is a kind of dimensionality reduction method whereas factor analysis is the latent variable method.\n",
    "- PCA is a type of factor analysis. PCA is observational whereas FA is a modeling technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<a href=\"#Content\">Back to Content</a>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#found errors\": Dataset is not available"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
