{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7719b066",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a22f2b3",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "https://www.mygreatlearning.com/blog/multinomial-naive-bayes-explained/\n",
    "\n",
    "https://www.geeksforgeeks.org/applying-multinomial-naive-bayes-to-nlp-problems/\n",
    "\n",
    "https://www.ritchieng.com/machine-learning-multinomial-naive-bayes-vectorization/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a71c6c",
   "metadata": {},
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef63ebfa",
   "metadata": {},
   "source": [
    "1. <a href = \"#1.-What-is-Multinomial-Naive-Bayes?\">What is Multinomial Naive Bayes?</a>\n",
    "2. <a href = \"#2.-What-is-The-Bag-of-Words-Model?\">What is The Bag of Words Model?</a>\n",
    "3. <a href = \"#3.-Difference-between-Bernoulli,-Multinomial-and-Gaussian-Naive-Bayes\">Difference between Bernoulli, Multinomial and Gaussian Naive Bayes</a>\n",
    "4. <a href = \"#4.-Applying-Multinomial-Naive-Bayes-to-NLP-Problems\">Applying Multinomial Naive Bayes to NLP Problems</a>\n",
    "5. <a href = \"#5.-Advantages\">Advantages</a>\n",
    "6. <a href = \"#6.-Disadvantages\">Disadvantages</a>\n",
    "7. <a href = \"#7.-Application\">Application</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfd156e",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19abe9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2767a4e",
   "metadata": {},
   "source": [
    "## 1. What is Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fc7b7a",
   "metadata": {},
   "source": [
    "With an ever-growing amount of textual information stored in electronic form such as legal documents, policies, company strategies, etc., automatic text classification is becoming increasingly important. This requires a supervised learning technique that classifies every new document by assigning one or more class labels from a fixed or predefined class. It uses the bag of words approach, where the individual words in the document constitute its features, and the order of the words is ignored. This technique is different from the way we communicate with each other. It treats the language like it’s just a bag full of words and each message is a random handful of them. Large documents have a lot of words that are generally characterized by very high dimensionality feature space with thousands of features. Hence, the learning algorithm requires to tackle high dimensional problems, both in terms of classification performance and computational speed. \n",
    "\n",
    "Naïve Bayes, which is computationally very efficient and easy to implement, is a learning algorithm frequently used in text classification problems. Two event models are commonly used: \n",
    "\n",
    "- Multivariate Bernoulli Event Model\n",
    "- Multivariate Event Model\n",
    "\n",
    "The Multivariate Event model is referred to as Multinomial Naive Bayes.\n",
    "When most people want to learn about Naive Bayes, they want to learn about the Multinomial Naive Bayes Classifier. However, there is another commonly used version of Naïve Bayes, called Gaussian Naive Bayes Classification.\n",
    "\n",
    "Naive Bayes is based on Bayes’ theorem, where the adjective Naïve says that features in the dataset are mutually independent. Occurrence of one feature does not affect the probability of occurrence of the other feature. For small sample sizes, Naïve Bayes can outperform the most powerful alternatives. Being relatively robust, easy to implement, fast, and accurate, it is used in many different fields.\n",
    "\n",
    "For Example, Spam filtering in email, Diagnosis of diseases, making decisions about treatment, Classification of RNA sequences in taxonomic studies, to name a few. However, we have to keep in mind about the type of data and the type of problem to be solved that dictates which classification model we want to choose. Strong violations of the independence assumptions and non-classification problems can lead to poor performance. In practice, it is recommended to use different classification models on the same dataset and then consider the performance, as well as computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e887092",
   "metadata": {},
   "source": [
    "[<a href=\"#Content\">Back to Content</a>]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dad119a",
   "metadata": {},
   "source": [
    "## 2. What is The Bag of Words Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515f6344",
   "metadata": {},
   "source": [
    "Feature extraction and Selection are the most important sub-tasks in pattern classification. The three main criteria of good features are:\n",
    "\n",
    "Salient: The features should be meaningful and important to the problem\n",
    "Invariant: The features are resistant to scaling, distortion and orientation etc. \n",
    "Discriminatory:  For training of classifiers, the features should have enough information to distinguish between patterns.\n",
    "Bag of words is a commonly used model in Natural Language Processing. The idea behind this model is the creation of vocabulary that contains the collection of different words, and each word is associated with a count of how it occurs. Later, the vocabulary is used to create d-dimensional feature vectors.\n",
    "\n",
    "For Example: \n",
    "\n",
    "D1: Each country has its own constitution\n",
    "D2: Every country has its own uniqueness\n",
    "\n",
    "Vocabulary could be written as:\n",
    "V= {each : 1, state : 1, has : 2, its : 2, own : 2, constitution : 1, every : 1, country : 2,} \n",
    "\n",
    "**Tokenization**\n",
    "\n",
    "It is the process of breaking down the text corpus into individual elements. These individual elements act as an input to machine learning algorithms.\n",
    "\n",
    "For Example:\n",
    "\n",
    "Every country has its own uniqueness \n",
    "\n",
    "\n",
    "Stop Words\n",
    "Stop Words also known as un-informative words such as (so, and, or, the) should be removed from the document.\n",
    "\n",
    "**Stemming and Lemmatization**\n",
    "\n",
    "Stemming and Lemmatization are the process of transforming a word into its root form and aims to obtain the grammatically correct forms of words.\n",
    "\n",
    "The above-mentioned process comes under the Bag of Words Model. Multinomial Naïve Bayes uses term frequency i.e. the number of times a given term appears in a document. Term frequency is often normalized by dividing the raw term frequency by the document length. After normalization, term frequency can be used to compute maximum likelihood estimates based on the training data to estimate the conditional probability.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let me explain a Multinomial Naïve Bayes Classifier where we want to filter out the spam messages. Initially, we consider eight normal messages and four spam messages. Now, imagine we received a lot of emails from friends, family, office and we also received spam (unwanted messages that are usually scams or unsolicited advertisements). \n",
    "\n",
    "Let see the histogram of all the words that occur in the normal messages from family and friends.\n",
    "\n",
    "We can use the histogram to calculate the probabilities of seeing each word, given that it was a normal message. The probability of word dear given that we saw in normal message is-\n",
    "\n",
    "Probability (Dear|Normal) = 8 /17 = 0.47\n",
    "\n",
    "Similarly, the probability of word Friend is-\n",
    "\n",
    "Probability (Friend/Normal) = 5/ 17 =0.29\n",
    "Probability (Lunch/Normal) = 3/ 17 =0.18\n",
    "Probability (Money/Normal) = 1/ 17 =0.06\n",
    "\n",
    "Now let’s make the histogram of all the words in spam.\n",
    "\n",
    "The probability of word dear given that we saw in spam message is-\n",
    "\n",
    "Probability (Dear|Spam) = 2 /7 = 0.29\n",
    "\n",
    "Similarly, the probability of word Friend is-\n",
    "\n",
    "Probability (Friend/Spam) = 1/ 7 =0.14\n",
    "Probability (Lunch/Spam) = 0/ 7 =0.00\n",
    "Probability (Money/Spam) = 4/ 7 =0.57\n",
    "\n",
    "Here, we have calculated the probabilities of discrete words and not the probability of something continuous like weight or height. These Probabilities are also called Likelihoods.\n",
    "\n",
    "Now, let’s say we have received a normal message as Dear Friend and we want to find out if it’s a normal message or spam. \n",
    "\n",
    "We start with an initial guess that any message is a Normal Message.\n",
    "\n",
    "From our initial assumptions of 8 Normal messages and 4 Spam messages, 8 out of 12 messages are normal messages. The prior probability, in this case, will be:\n",
    "\n",
    "Probability (Normal) = 8 / (8+4) = 0.67\n",
    "\n",
    "We multiply this prior probability with the probabilities of Dear Friend that we have calculated earlier.\n",
    "\n",
    "0.67 * 0.47 * 0.29 = 0.09\n",
    "\n",
    "0.09 is the probability score considering Dear Friend is a normal message. \n",
    "\n",
    "Alternatively, let’s say that any message is a Spam.\n",
    "\n",
    "4 out of 12 messages are Spam. The prior probability in this case will be:\n",
    "\n",
    "Probability (Normal) = 4 / (8+4) = 0.33\n",
    "\n",
    "Now we multiply the prior probability with the probabilities of Dear Friend that we have calculated earlier.\n",
    "\n",
    "0.33 * 0.29 * 0.14 = 0.01\n",
    "\n",
    "0.01 is the probability score considering Dear Friend is a Spam. \n",
    "\n",
    "The probability score of Dear Friend being a normal message is greater than the probability score of Dear Friend being spam. We can conclude that Dear Friend is a normal message.\n",
    "\n",
    "Naive Bayes treats all words equally regardless of how they are placed because it’s difficult to keep track of every single reasonable phrase in a language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3d055a",
   "metadata": {},
   "source": [
    "[<a href=\"#Content\">Back to Content</a>]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3eac53",
   "metadata": {},
   "source": [
    "## 3. Difference between Bernoulli, Multinomial and Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cafe055",
   "metadata": {},
   "source": [
    "Multinomial Naïve Bayes consider a feature vector where a given term represents the number of times it appears or very often i.e. frequency. On the other hand, Bernoulli is a binary algorithm used when the feature is present or not. At last Gaussian is based on continuous distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4cdb29",
   "metadata": {},
   "source": [
    "[<a href=\"#Content\">Back to Content</a>]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4e2cf8",
   "metadata": {},
   "source": [
    "## 4. Applying Multinomial Naive Bayes to NLP Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e09759",
   "metadata": {},
   "source": [
    "**Objective:** To classify the sms whether it is positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14b49aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative: read file into pandas from a URL\n",
    "url = 'https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv'\n",
    "sms = pd.read_table(url, header=None, names=['label', 'message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c84e221e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the shape\n",
    "sms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "158a307e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the first 10 rows\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f386954d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the class distribution\n",
    "sms.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aedf9529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  label_num\n",
       "0   ham  Go until jurong point, crazy.. Available only ...          0\n",
       "1   ham                      Ok lar... Joking wif u oni...          0\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...          1\n",
       "3   ham  U dun say so early hor... U c already then say...          0\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...          0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert label to a numerical variable\n",
    "sms['label_num'] = sms.label.map({'ham':0, 'spam':1})\n",
    "# check that the conversion worked\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35866278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572,)\n",
      "(5572,)\n"
     ]
    }
   ],
   "source": [
    "# how to define X and y (from the SMS data) for use with COUNTVECTORIZER\n",
    "X = sms.message\n",
    "y = sms.label_num\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d20cb629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179,)\n",
      "(1393,)\n",
      "(4179,)\n",
      "(1393,)\n"
     ]
    }
   ],
   "source": [
    "# split X and y into training and testing sets\n",
    "# by default, it splits 75% training and 25% test\n",
    "# random_state=1 for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b2657b",
   "metadata": {},
   "source": [
    "**Why are we splitting into training and testing sets before vectorizing?**\n",
    "\n",
    "Background of train/test split\n",
    "\n",
    "- Train/test split is for model evaluation\n",
    "    - Model evaluation is to simulate the future\n",
    "    - Past data is exchangeable for future data\n",
    "    - We pretend some of our past data is coming into our future data\n",
    "    - By training, predicting and evaluating the data, we can check the performance of our model\n",
    "\n",
    "Vectorize then split\n",
    "\n",
    "- If we vectorize then we train/test split, our document-term matrix would contain every single feature (word) in the test and training sets\n",
    "    - What we want is to simulate the real world\n",
    "    - We would always see words we have not seen before so this method is not realistic and we cannot properly evaluate our models\n",
    "\n",
    "Split then vectorize (correct way)\n",
    "\n",
    "- We do the train/test split before the CountVectorizer to properly simulate the real world where our future data contains words we have not seen before\n",
    "\n",
    "After you train your data and chose the best model, you would then train on all of your data before predicting actual future data to maximize learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688ee21a",
   "metadata": {},
   "source": [
    "**Vectorizing our dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3c1388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. instantiate the vectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f87e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn training data vocabulary, then use it to create a document-term matrix\n",
    "\n",
    "# 3. fit\n",
    "vect.fit(X_train)\n",
    "\n",
    "# 4. transform training data\n",
    "X_train_dtm = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "848a5106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x7456 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 55209 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# equivalently: combine fit and transform into a single step\n",
    "# this is faster and what most people would do\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "# examine the document-term matrix\n",
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8cb1c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1393x7456 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 17604 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm\n",
    "\n",
    "# you can see that the number of columns, 7456, is the same as what we have learned above in X_train_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393d162e",
   "metadata": {},
   "source": [
    "**Building and evaluating a model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "867920c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. instantiate a Multinomial Naive Bayes model\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95b9ecf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 4.01 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. train the model \n",
    "# using X_train_dtm (timing it with an IPython \"magic command\")\n",
    "\n",
    "%time nb.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e306d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. make class predictions for X_test_dtm\n",
    "y_pred_class = nb.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0bfdb24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9885139985642498"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy of class predictions\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71747437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1203,    5],\n",
       "       [  11,  174]], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf9263c",
   "metadata": {},
   "source": [
    "Confusion matrix:\n",
    "\n",
    "[TN FP\n",
    "\n",
    "FN TP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72a68f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "574               Waiting for your call.\n",
       "3375             Also andros ice etc etc\n",
       "45      No calls..messages..missed calls\n",
       "3415             No pic. Please re-send.\n",
       "1988    No calls..messages..missed calls\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print message text for the false positives (ham incorrectly classified as spam)\n",
    "\n",
    "X_test[y_pred_class > y_test]\n",
    "\n",
    "# alternative less elegant but easier to understand\n",
    "# X_test[(y_pred_class==1) & (y_test==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d44367ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3132    LookAtMe!: Thanks for your purchase of a video...\n",
       "5       FreeMsg Hey there darling it's been 3 week's n...\n",
       "3530    Xmas & New Years Eve tickets are now on sale f...\n",
       "684     Hi I'm sue. I am 20 years old and work as a la...\n",
       "1875    Would you like to see my XXX pics they are so ...\n",
       "1893    CALL 09090900040 & LISTEN TO EXTREME DIRTY LIV...\n",
       "4298    thesmszone.com lets you send free anonymous an...\n",
       "4949    Hi this is Amy, we will be sending you a free ...\n",
       "2821    INTERFLORA - It's not too late to order Inter...\n",
       "2247    Hi ya babe x u 4goten bout me?' scammers getti...\n",
       "4514    Money i have won wining number 946 wot do i do...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print message text for the false negatives (spam incorrectly classified as ham)\n",
    "\n",
    "X_test[y_pred_class < y_test]\n",
    "# alternative less elegant but easier to understand\n",
    "# X_test[(y_pred_class=0) & (y_test=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa177ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.87744864e-03, 1.83488846e-05, 2.07301295e-03, ...,\n",
       "       1.09026171e-06, 1.00000000e+00, 3.98279868e-09])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate predicted probabilities for X_test_dtm (poorly calibrated)\n",
    "\n",
    "# Numpy Array with 2C\n",
    "# left Column: probability class 0\n",
    "# right C: probability class 1\n",
    "# we only need the right column \n",
    "y_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\n",
    "y_pred_prob\n",
    "\n",
    "# Naive Bayes predicts very extreme probabilites, you should not take them at face value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7015a040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9866431000536962"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate AUC\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abaefd4",
   "metadata": {},
   "source": [
    "- AUC is useful as a single number summary of classifier performance\n",
    "- Higher value = better classifier\n",
    "- If you randomly chose one positive and one negative observation, AUC represents the likelihood that your classifier will assign a higher predicted probability to the positive observation\n",
    "- AUC is useful even when there is high class imbalance (unlike classification accuracy)\n",
    "        - Fraud case\n",
    "        - Null accuracy almost 99%\n",
    "        - AUC is useful here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15754f22",
   "metadata": {},
   "source": [
    "[<a href=\"#Content\">Back to Content</a>]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d36228b",
   "metadata": {},
   "source": [
    "## 5. Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd40ed96",
   "metadata": {},
   "source": [
    "1. Low computation cost.\n",
    "2. It can effectively work with large datasets.\n",
    "3. For small sample sizes, Naive Bayes can outperform the most powerful alternatives.\n",
    "4. Easy to implement, fast and accurate method of prediction.\n",
    "5. Can work with multiclass prediction problems.\n",
    "6. It performs well in text classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0ef044",
   "metadata": {},
   "source": [
    "[<a href=\"#Content\">Back to Content</a>]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7260cd",
   "metadata": {},
   "source": [
    "## 6. Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b740b951",
   "metadata": {},
   "source": [
    "It is very difficult to get the set of independent predictors for developing a model using Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66b4da3",
   "metadata": {},
   "source": [
    "[<a href=\"#Content\">Back to Content</a>]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d030884",
   "metadata": {},
   "source": [
    "## 7. Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47af5f59",
   "metadata": {},
   "source": [
    "1. Naive Bayes classifier is used in Text Classification, Spam filtering and Sentiment Analysis. It has a higher success rate.\n",
    "2. than other algorithms.\n",
    "3. Naïve Bayes along with Collaborative filtering are used in Recommended Systems.\n",
    "4. It is also used in disease prediction based on health parameters.\n",
    "5. This algorithm has also found its application in Face recognition.\n",
    "6. Naive Bayes is used in prediction of weather reports based on atmospheric conditions (temp, wind, clouds, humidity etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9153dcd",
   "metadata": {},
   "source": [
    "[<a href=\"#Content\">Back to Content</a>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa53fe4d-184c-4db9-a7df-a841a38af406",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no errors:Tested"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
